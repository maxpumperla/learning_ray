{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280da2ec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_create[]\n",
    "import ray\n",
    "\n",
    "# Create a dataset containing integers in the range [0, 10000).\n",
    "ds = ray.data.range(10000)\n",
    "\n",
    "# Basic operations: show the size of the dataset, get a few samples, print the schema.\n",
    "print(ds.count())  # -> 10000\n",
    "print(ds.take(5))  # -> [0, 1, 2, 3, 4]\n",
    "print(ds.schema())  # -> <class 'int'>\n",
    "# end::ds_create[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99261aa4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_read_write[]\n",
    "# Save the dataset to a local file and load it back.\n",
    "ray.data.range(10000).write_csv(\"local_dir\")\n",
    "ds = ray.data.read_csv(\"local_dir\")\n",
    "print(ds.count())\n",
    "# end::ds_read_write[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dcf8f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_transform[]\n",
    "# Basic transformations: join two datasets, filter, and sort.\n",
    "ds1 = ray.data.range(10000)\n",
    "ds2 = ray.data.range(10000)\n",
    "ds3 = ds1.union(ds2)\n",
    "print(ds3.count())  # -> 20000\n",
    "\n",
    "# Filter the combined dataset to only the even elements.\n",
    "ds3 = ds3.filter(lambda x: x % 2 == 0)\n",
    "print(ds3.count())  # -> 10000\n",
    "print(ds3.take(5))  # -> [0, 2, 4, 6, 8]\n",
    "\n",
    "# Sort the filtered dataset.\n",
    "ds3 = ds3.sort()\n",
    "print(ds3.take(5))  # -> [0, 0, 2, 2, 4]\n",
    "# end::ds_transform[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e395f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_repartition[]\n",
    "ds1 = ray.data.range(10000)\n",
    "print(ds1.num_blocks())  # -> 200\n",
    "ds2 = ray.data.range(10000)\n",
    "print(ds2.num_blocks())  # -> 200\n",
    "ds3 = ds1.union(ds2)\n",
    "print(ds3.num_blocks())  # -> 400\n",
    "\n",
    "print(ds3.repartition(200).num_blocks())  # -> 200\n",
    "# end::ds_repartition[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8af506",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_schema_1[]\n",
    "ds = ray.data.from_items([{\"id\": \"abc\", \"value\": 1}, {\"id\": \"def\", \"value\": 2}])\n",
    "print(ds.schema())  # -> id: string, value: int64\n",
    "# end::ds_schema_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb57da1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_schema_2[]\n",
    "pandas_df = ds.to_pandas()  # pandas_df will inherit the schema from our Dataset.\n",
    "# end::ds_schema_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429e82e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_compute_1[]\n",
    "ds = ray.data.range(10000).map(lambda x: x ** 2)\n",
    "ds.take(5)  # -> [0, 1, 4, 9, 16]\n",
    "# end::ds_compute_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a069cb3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_compute_2[]\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ds = ray.data.range(10000).map_batches(lambda batch: np.square(batch).tolist())\n",
    "ds.take(5)  # -> [0, 1, 4, 9, 16]\n",
    "# end::ds_compute_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bfd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ds_compute_3[]\n",
    "def load_model():\n",
    "    # Return a dummy model just for this example.\n",
    "    # In reality, this would likely load some model weights onto a GPU.\n",
    "    class DummyModel:\n",
    "        def __call__(self, batch):\n",
    "            return batch\n",
    "\n",
    "    return DummyModel()\n",
    "\n",
    "class MLModel:\n",
    "    def __init__(self):\n",
    "        # load_model() will only run once per actor that's started.\n",
    "        self._model = load_model()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self._model(batch)\n",
    "\n",
    "\n",
    "ds.map_batches(MLModel, compute=\"actors\")\n",
    "# end::ds_compute_3[]\n",
    "\n",
    "# TODO how can we make this more concrete?\n",
    "cpu_intensive_preprocessing = lambda batch: batch\n",
    "gpu_intensive_inference = lambda batch: batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef875ab1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_pipeline_1[]\n",
    "ds = ray.data.read_parquet(\"s3://my_bucket/input_data\")\\\n",
    "        .map(cpu_intensive_preprocessing)\\\n",
    "        .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)\\\n",
    "        .repartition(10)\\\n",
    "        .write_parquet(\"s3://my_bucket/output_predictions\")\n",
    "# end::ds_pipeline_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236ac98",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_pipeline_2[]\n",
    "ds = ray.data.read_parquet(\"s3://my_bucket/input_data\")\\\n",
    "        .window(blocks_per_window=5)\\\n",
    "        .map(cpu_intensive_preprocessing)\\\n",
    "        .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)\\\n",
    "        .repartition(10)\\\n",
    "        .write_parquet(\"s3://my_bucket/output_predictions\")\n",
    "# end::ds_pipeline_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_1[]\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "@ray.remote\n",
    "class TrainingWorker:\n",
    "    def __init__(self, alpha: float):\n",
    "        self._model = SGDClassifier(alpha=alpha)\n",
    "\n",
    "    def train(self, train_shard: ray.data.Dataset):\n",
    "        for i, epoch in enumerate(train_shard.iter_epochs()):\n",
    "            X, Y = zip(*list(epoch.iter_rows()))\n",
    "            self._model.partial_fit(X, Y, classes=[0, 1])\n",
    "\n",
    "        return self._model\n",
    "\n",
    "    def test(self, X_test: np.ndarray, Y_test: np.ndarray):\n",
    "        return self._model.score(X_test, Y_test)\n",
    "# end::parallel_sgd_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442edb38",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_2[]\n",
    "ALPHA_VALS = [0.00008, 0.00009, 0.0001, 0.00011, 0.00012] \n",
    "\n",
    "print(f\"Starting {len(ALPHA_VALS)} training workers.\")\n",
    "workers = [TrainingWorker.remote(alpha) for alpha in ALPHA_VALS]\n",
    "# end::parallel_sgd_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1cce6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_3[]\n",
    "# Generate training & validation data for a classification problem.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(*datasets.make_classification())\n",
    "\n",
    "# Create a dataset pipeline out of the training data. The data will be randomly\n",
    "# shuffled and split across the workers for 10 iterations.\n",
    "train_ds = ray.data.from_items(list(zip(X_train, Y_train)))\n",
    "shards = train_ds.repeat(10)\\\n",
    "                 .random_shuffle_each_window()\\\n",
    "                 .split(len(workers), locality_hints=workers)\n",
    "# end::parallel_sgd_3[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c1bed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_4[]\n",
    "# Wait for training to complete on all of the workers.\n",
    "ray.get([worker.train.remote(shard) for worker, shard in zip(workers, shards)])\n",
    "# end::parallel_sgd_4[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729f7bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_5[]\n",
    "# Get validation results from each worker.\n",
    "print(ray.get([worker.test.remote(X_test, Y_test) for worker in workers]))\n",
    "# end::parallel_sgd_5[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74889dc1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_1[]\n",
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "\n",
    "ray.init()  # Start or connect to Ray.\n",
    "enable_dask_on_ray()  # Enable the Ray scheduler backend for Dask.\n",
    "# end::dask_on_ray_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cbee72",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_2[]\n",
    "import dask\n",
    "\n",
    "df = dask.datasets.timeseries()\n",
    "df = df[df.y > 0].groupby(\"name\").x.std()\n",
    "df.compute()  # Trigger the task graph to be evaluated.\n",
    "# end::dask_on_ray_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a374310",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_3[]\n",
    "import ray\n",
    "ds = ray.data.range(10000)\n",
    "\n",
    "# Convert the Dataset to a Dask DataFrame.\n",
    "df = ds.to_dask()\n",
    "print(df.std().compute())  # -> 2886.89568\n",
    "\n",
    "# Convert the Dask DataFrame back to a Dataset.\n",
    "ds = ray.data.from_dask(df)\n",
    "print(ds.std())  # -> 2886.89568\n",
    "# end::dask_on_ray_3[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_preprocess[]\n",
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "LABEL_COLUMN = \"is_big_tip\"\n",
    "\n",
    "enable_dask_on_ray()\n",
    "\n",
    "\n",
    "def load_dataset(path: str, *, include_label=True):\n",
    "    # Load the data and drop unused columns.\n",
    "    df = dd.read_csv(path, assume_missing=True,\n",
    "                     usecols=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "                              \"passenger_count\", \"trip_distance\", \"fare_amount\",\n",
    "                              \"tip_amount\"])\n",
    "\n",
    "    # Basic cleaning, drop nulls and outliers.\n",
    "    df = df.dropna()\n",
    "    df = df[(df[\"passenger_count\"] <= 4) &\n",
    "            (df[\"trip_distance\"] < 100) &\n",
    "            (df[\"fare_amount\"] < 1000)]\n",
    "\n",
    "    # Convert datetime strings to datetime objects.\n",
    "    df[\"tpep_pickup_datetime\"] = dd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "    df[\"tpep_dropoff_datetime\"] = dd.to_datetime(df[\"tpep_dropoff_datetime\"])\n",
    "\n",
    "    # Add three new features: trip duration, hour the trip started, and day of the week.\n",
    "    df[\"trip_duration\"] = (df[\"tpep_dropoff_datetime\"] -\n",
    "                           df[\"tpep_pickup_datetime\"]).dt.seconds\n",
    "    df = df[df[\"trip_duration\"] < 4 * 60 * 60] # 4 hours.\n",
    "    df[\"hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"tpep_pickup_datetime\"].dt.weekday\n",
    "\n",
    "    if include_label:\n",
    "        # Calculate label column: if tip was more or less than 20% of the fare.\n",
    "        df[LABEL_COLUMN] = df[\"tip_amount\"] > 0.2 * df[\"fare_amount\"]\n",
    "\n",
    "    # Drop unused columns.\n",
    "    df = df.drop(\n",
    "        columns=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\"]\n",
    "    )\n",
    "\n",
    "    return ray.data.from_dask(df)\n",
    "# end::ml_pipeline_preprocess[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_model[]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NUM_FEATURES = 6\n",
    "\n",
    "\n",
    "class FarePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(NUM_FEATURES, 256)\n",
    "        self.fc2 = nn.Linear(256, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "\n",
    "    def forward(self, *x):\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "\n",
    "        return x\n",
    "# end::ml_pipeline_model[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_train_1[]\n",
    "import ray.train as train\n",
    "\n",
    "\n",
    "def train_epoch(iterable_dataset, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for X, y in iterable_dataset:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Compute prediction error.\n",
    "        pred = torch.round(model(X.float()))\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "# end::ml_pipeline_train_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e6e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_train_2[]\n",
    "def validate_epoch(iterable_dataset, model, loss_fn, device):\n",
    "    num_batches = 0\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in iterable_dataset:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            num_batches += 1\n",
    "            pred = torch.round(model(X.float()))\n",
    "            loss += loss_fn(pred, y).item()\n",
    "    loss /= num_batches\n",
    "    result = {\"loss\": loss}\n",
    "    return result\n",
    "# end::ml_pipeline_train_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32081b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_train_3[]\n",
    "def train_func(config):\n",
    "    batch_size = config.get(\"batch_size\", 32)\n",
    "    lr = config.get(\"lr\", 1e-2)\n",
    "    epochs = config.get(\"epochs\", 3)\n",
    "\n",
    "    train_dataset_pipeline_shard = train.get_dataset_shard(\"train\")\n",
    "    validation_dataset_pipeline_shard = train.get_dataset_shard(\"validation\")\n",
    "\n",
    "    model = train.torch.prepare_model(FarePredictor())\n",
    "\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_dataset_iterator = train_dataset_pipeline_shard.iter_epochs()\n",
    "    validation_dataset_iterator = \\\n",
    "        validation_dataset_pipeline_shard.iter_epochs()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_dataset = next(train_dataset_iterator)\n",
    "        validation_dataset = next(validation_dataset_iterator)\n",
    "\n",
    "        train_torch_dataset = train_dataset.to_torch(\n",
    "            label_column=LABEL_COLUMN,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        validation_torch_dataset = validation_dataset.to_torch(\n",
    "            label_column=LABEL_COLUMN,\n",
    "            batch_size=batch_size)\n",
    "\n",
    "        device = train.torch.get_device()\n",
    "\n",
    "        train_epoch(train_torch_dataset, model, loss_fn, optimizer, device)\n",
    "        result = validate_epoch(validation_torch_dataset, model, loss_fn,\n",
    "                                device)\n",
    "        train.report(**result)\n",
    "        train.save_checkpoint(epoch=epoch, model_weights=model.module.state_dict())\n",
    "# end::ml_pipeline_train_3[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_train_4[]\n",
    "def get_training_datasets(*, test_pct=0.8):\n",
    "    ds = load_dataset(\"nyc_tlc_data/yellow_tripdata_2020-01.csv\")\n",
    "    ds, _ = ds.split_at_indices([int(0.01 * ds.count())])\n",
    "    train_ds, test_ds = ds.split_at_indices([int(test_pct * ds.count())])\n",
    "    train_ds_pipeline = train_ds.repeat().random_shuffle_each_window()\n",
    "    test_ds_pipeline = test_ds.repeat()\n",
    "    return {\"train\": train_ds_pipeline, \"validation\": test_ds_pipeline}\n",
    "# end::ml_pipeline_train_4[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630414c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_train_5[]\n",
    "trainer = train.Trainer(\"torch\", num_workers=4)\n",
    "config = {\"lr\": 1e-2, \"epochs\": 3, \"batch_size\": 64}\n",
    "trainer.start()\n",
    "trainer.run(train_func, config, dataset=get_training_datasets())\n",
    "model_weights = trainer.latest_checkpoint.get(\"model_weights\")\n",
    "trainer.shutdown()\n",
    "# end::ml_pipeline_train_5[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_inference[]\n",
    "class InferenceWrapper:\n",
    "    def __init__(self):\n",
    "        self._model = FarePredictor()\n",
    "        self._model.load_state_dict(model_weights)\n",
    "        self._model.eval()\n",
    "\n",
    "    def __call__(self, df):\n",
    "        tensor = torch.as_tensor(df.to_numpy(), dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            predictions = torch.round(self._model(tensor))\n",
    "        df[LABEL_COLUMN] = predictions.numpy()\n",
    "        return df\n",
    "\n",
    "\n",
    "ds = load_dataset(\"nyc_tlc_data/yellow_tripdata_2021-01.csv\", include_label=False)\n",
    "ds.map_batches(InferenceWrapper, compute=\"actors\").write_csv(\"output\")\n",
    "# end::ml_pipeline_inference[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
