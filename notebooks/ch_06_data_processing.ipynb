{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0177e06",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_create[]\n",
    "import ray\n",
    "\n",
    "# Create a dataset containing integers in the range [0, 10000).\n",
    "ds = ray.data.range(10000)\n",
    "\n",
    "# Basic operations: show the size of the dataset, get a few samples, print the schema.\n",
    "print(ds.count())  # -> 10000\n",
    "print(ds.take(5))  # -> [0, 1, 2, 3, 4]\n",
    "print(ds.schema())  # -> <class 'int'>\n",
    "# end::ds_create[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17192aa0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_read_write[]\n",
    "# Save the dataset to a local file and load it back.\n",
    "ray.data.range(10000).write_csv(\"local_dir\")\n",
    "ds = ray.data.read_csv(\"local_dir\")\n",
    "print(ds.count())\n",
    "# end::ds_read_write[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe56fdd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_transform[]\n",
    "# Basic transformations: join two datasets, filter, and sort.\n",
    "ds1 = ray.data.range(10000)\n",
    "ds2 = ray.data.range(10000)\n",
    "ds3 = ds1.union(ds2)\n",
    "print(ds3.count())  # -> 20000\n",
    "\n",
    "# Filter the combined dataset to only the even elements.\n",
    "ds3 = ds3.filter(lambda x: x % 2 == 0)\n",
    "print(ds3.count())  # -> 10000\n",
    "print(ds3.take(5))  # -> [0, 2, 4, 6, 8]\n",
    "\n",
    "# Sort the filtered dataset.\n",
    "ds3 = ds3.sort()\n",
    "print(ds3.take(5))  # -> [0, 0, 2, 2, 4]\n",
    "# end::ds_transform[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee04a49",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_repartition[]\n",
    "ds1 = ray.data.range(10000)\n",
    "print(ds1.num_blocks())  # -> 200\n",
    "ds2 = ray.data.range(10000)\n",
    "print(ds2.num_blocks())  # -> 200\n",
    "ds3 = ds1.union(ds2)\n",
    "print(ds3.num_blocks())  # -> 400\n",
    "\n",
    "print(ds3.repartition(200).num_blocks())  # -> 200\n",
    "# end::ds_repartition[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ea9ad",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_schema_1[]\n",
    "ds = ray.data.from_items([{\"id\": \"abc\", \"value\": 1}, {\"id\": \"def\", \"value\": 2}])\n",
    "print(ds.schema())  # -> id: string, value: int64\n",
    "# end::ds_schema_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7b233",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_schema_2[]\n",
    "pandas_df = ds.to_pandas()  # pandas_df will inherit the schema from our Dataset.\n",
    "# end::ds_schema_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f2ba0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_compute_1[]\n",
    "ds = ray.data.range(10000).map(lambda x: x ** 2)\n",
    "ds.take(5)  # -> [0, 1, 4, 9, 16]\n",
    "# end::ds_compute_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef9bfa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_compute_2[]\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ds = ray.data.range(10000).map_batches(lambda batch: np.square(batch).tolist())\n",
    "ds.take(5)  # -> [0, 1, 4, 9, 16]\n",
    "# end::ds_compute_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ds_compute_3[]\n",
    "def load_model():\n",
    "    # Returns a dummy model for this example.\n",
    "    # In reality, this would likely load some model weights onto a GPU.\n",
    "    class DummyModel:\n",
    "        def __call__(self, batch):\n",
    "            return batch\n",
    "\n",
    "    return DummyModel()\n",
    "\n",
    "\n",
    "class MLModel:\n",
    "    def __init__(self):\n",
    "        # load_model() will only run once per actor that's started.\n",
    "        self._model = load_model()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self._model(batch)\n",
    "\n",
    "\n",
    "ds.map_batches(MLModel, compute=\"actors\")\n",
    "# end::ds_compute_3[]\n",
    "\n",
    "\n",
    "cpu_intensive_preprocessing = lambda batch: batch\n",
    "gpu_intensive_inference = lambda batch: batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c74cf5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_pipeline_1[]\n",
    "ds = ray.data.read_parquet(\"s3://my_bucket/input_data\")\\\n",
    "        .map(cpu_intensive_preprocessing)\\\n",
    "        .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)\\\n",
    "        .repartition(10)\\\n",
    "        .write_parquet(\"s3://my_bucket/output_predictions\")\n",
    "# end::ds_pipeline_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51367151",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_pipeline_2[]\n",
    "ds = ray.data.read_parquet(\"s3://my_bucket/input_data\")\\\n",
    "        .window(blocks_per_window=5)\\\n",
    "        .map(cpu_intensive_preprocessing)\\\n",
    "        .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)\\\n",
    "        .repartition(10)\\\n",
    "        .write_parquet(\"s3://my_bucket/output_predictions\")\n",
    "# end::ds_pipeline_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_1[]\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "@ray.remote\n",
    "class TrainingWorker:\n",
    "    def __init__(self, alpha: float):\n",
    "        self._model = SGDClassifier(alpha=alpha)\n",
    "\n",
    "    def train(self, train_shard: ray.data.Dataset):\n",
    "        for i, epoch in enumerate(train_shard.iter_epochs()):\n",
    "            X, Y = zip(*list(epoch.iter_rows()))\n",
    "            self._model.partial_fit(X, Y, classes=[0, 1])\n",
    "\n",
    "        return self._model\n",
    "\n",
    "    def test(self, X_test: np.ndarray, Y_test: np.ndarray):\n",
    "        return self._model.score(X_test, Y_test)\n",
    "# end::parallel_sgd_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e75f7f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_2[]\n",
    "ALPHA_VALS = [0.00008, 0.00009, 0.0001, 0.00011, 0.00012] \n",
    "\n",
    "print(f\"Starting {len(ALPHA_VALS)} training workers.\")\n",
    "workers = [TrainingWorker.remote(alpha) for alpha in ALPHA_VALS]\n",
    "# end::parallel_sgd_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea829abb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_3[]\n",
    "# Generate training & validation data for a classification problem.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(*datasets.make_classification())\n",
    "\n",
    "# Create a dataset pipeline out of the training data. The data will be randomly\n",
    "# shuffled and split across the workers for 10 iterations.\n",
    "train_ds = ray.data.from_items(list(zip(X_train, Y_train)))\n",
    "shards = train_ds.repeat(10)\\\n",
    "                 .random_shuffle_each_window()\\\n",
    "                 .split(len(workers), locality_hints=workers)\n",
    "\n",
    "# Wait for training to complete on all of the workers.\n",
    "ray.get([worker.train.remote(shard) for worker, shard in zip(workers, shards)])\n",
    "# end::parallel_sgd_3[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdfaa80",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_5[]\n",
    "# Get validation results from each worker.\n",
    "print(ray.get([worker.test.remote(X_test, Y_test) for worker in workers]))\n",
    "# end::parallel_sgd_5[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a999e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_1[]\n",
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "\n",
    "ray.init()  # Start or connect to Ray.\n",
    "enable_dask_on_ray()  # Enable the Ray scheduler backend for Dask.\n",
    "# end::dask_on_ray_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef3898",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_2[]\n",
    "import dask\n",
    "\n",
    "df = dask.datasets.timeseries()\n",
    "df = df[df.y > 0].groupby(\"name\").x.std()\n",
    "df.compute()  # Trigger the task graph to be evaluated.\n",
    "# end::dask_on_ray_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1791a04",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_3[]\n",
    "import ray\n",
    "ds = ray.data.range(10000)\n",
    "\n",
    "# Convert the Dataset to a Dask DataFrame.\n",
    "df = ds.to_dask()\n",
    "print(df.std().compute())  # -> 2886.89568\n",
    "\n",
    "# Convert the Dask DataFrame back to a Dataset.\n",
    "ds = ray.data.from_dask(df)\n",
    "print(ds.std())  # -> 2886.89568\n",
    "# end::dask_on_ray_3[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d34eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_preprocess[]\n",
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "LABEL_COLUMN = \"is_big_tip\"\n",
    "FEATURE_COLUMNS = [\"passenger_count\", \"trip_distance\", \"fare_amount\",\n",
    "                   \"trip_duration\", \"hour\", \"day_of_week\"]\n",
    "\n",
    "enable_dask_on_ray()\n",
    "\n",
    "\n",
    "def load_dataset(path: str, *, include_label=True):\n",
    "    # Load the data and drop unused columns.\n",
    "    columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\",\n",
    "               \"passenger_count\", \"trip_distance\", \"fare_amount\"]\n",
    "    df = dd.read_parquet(path, columns=columns)\n",
    "\n",
    "    # Basic cleaning, drop nulls and outliers.\n",
    "    df = df.dropna()\n",
    "    df = df[(df[\"passenger_count\"] <= 4) &\n",
    "            (df[\"trip_distance\"] < 100) &\n",
    "            (df[\"fare_amount\"] < 1000)]\n",
    "\n",
    "    # Convert datetime strings to datetime objects.\n",
    "    df[\"tpep_pickup_datetime\"] = dd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "    df[\"tpep_dropoff_datetime\"] = dd.to_datetime(df[\"tpep_dropoff_datetime\"])\n",
    "\n",
    "    # Add three new features: trip duration, hour the trip started,\n",
    "    # and day of the week.\n",
    "    df[\"trip_duration\"] = (df[\"tpep_dropoff_datetime\"] -\n",
    "                           df[\"tpep_pickup_datetime\"]).dt.seconds\n",
    "    df = df[df[\"trip_duration\"] < 4 * 60 * 60] # 4 hours.\n",
    "    df[\"hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"tpep_pickup_datetime\"].dt.weekday\n",
    "\n",
    "    if include_label:\n",
    "        # Calculate label column: if tip was more or less than 20% of the fare.\n",
    "        df[LABEL_COLUMN] = df[\"tip_amount\"] > 0.2 * df[\"fare_amount\"]\n",
    "\n",
    "    # Drop unused columns.\n",
    "    df = df.drop(\n",
    "        columns=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\"]\n",
    "    )\n",
    "\n",
    "    return ray.data.from_dask(df).repartition(100)\n",
    "# end::ml_pipeline_preprocess[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_train_1[]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ray.air import session\n",
    "from ray.air.config import ScalingConfig\n",
    "import ray.train as train\n",
    "from ray.train.torch import TorchCheckpoint, TorchTrainer\n",
    "\n",
    "from fare_predictor import FarePredictor\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config: dict):\n",
    "    batch_size = config.get(\"batch_size\", 32)\n",
    "    lr = config.get(\"lr\", 1e-2)\n",
    "    num_epochs = config.get(\"num_epochs\", 3)\n",
    "\n",
    "    dataset_shard = train.get_dataset_shard(\"train\")\n",
    "    model = train.torch.prepare_model(FarePredictor())\n",
    "\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in dataset_shard.iter_torch_batches(\n",
    "            batch_size=batch_size, dtypes=torch.float\n",
    "        ):\n",
    "            labels = torch.unsqueeze(batch[LABEL_COLUMN], dim=1)\n",
    "            inputs = torch.cat(\n",
    "                [torch.unsqueeze(batch[f], dim=1) for f in FEATURE_COLUMNS], dim=1\n",
    "            )\n",
    "            output = model(inputs)\n",
    "            batch_loss = loss_fn(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_batches += 1\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "        # loss /= num_batches\n",
    "        session.report(\n",
    "            {\"epoch\": epoch, \"loss\": loss},\n",
    "            checkpoint=TorchCheckpoint.from_model(model)\n",
    "        )\n",
    "# end::ml_pipeline_train_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadaba08",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_train_2[]\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\"lr\": 1e-2, \"num_epochs\": 3, \"batch_size\": 64},\n",
    "    scaling_config=ScalingConfig(num_workers=4),\n",
    "    datasets={\"train\": load_dataset(\"nyc_tlc_data/yellow_tripdata_2020-01.parquet\")},\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "trained_model = result.checkpoint\n",
    "# end::ml_pipeline_train_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_inference[]\n",
    "from ray.train.torch import TorchPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "\n",
    "batch_predictor = BatchPredictor(trained_model, TorchPredictor)\n",
    "ds = load_dataset(\n",
    "    \"nyc_tlc_data/yellow_tripdata_2021-01.parquet\", include_label=False)\n",
    "\n",
    "batch_predictor.predict_pipelined(ds, blocks_per_window=10)\n",
    "# end::ml_pipeline_inference[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
