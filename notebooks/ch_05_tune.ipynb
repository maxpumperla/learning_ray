{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed82bf7",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization with Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a719e",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_05_tune.ipynb).\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_05_tune.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc058b",
   "metadata": {},
   "source": [
    "For this chapter you need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray[tune]==2.2.0\"\n",
    "! pip install \"hyperopt==0.2.7\"\n",
    "! pip install \"bayesian-optimization==1.3.1\"\n",
    "! pip install \"tensorflow>=2.9.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0af473",
   "metadata": {},
   "source": [
    "\n",
    "To import utility files for this chapter, on Colab you will also have to clone\n",
    "the repo and copy the code files to the base path of the runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e740de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/maxpumperla/learning_ray\n",
    "%cp -r learning_ray/notebooks/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd95122",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from maze_gym_env import Environment\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class Policy:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A Policy suggests actions based on the current state.\n",
    "        We do this by tracking the value of each state-action pair.\n",
    "        \"\"\"\n",
    "        self.state_action_table = [\n",
    "            [0 for _ in range(env.action_space.n)]\n",
    "            for _ in range(env.observation_space.n)\n",
    "        ]\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def get_action(self, state, explore=True, epsilon=0.1):\n",
    "        \"\"\"Explore randomly or exploit the best value currently available.\"\"\"\n",
    "        if explore and random.uniform(0, 1) < epsilon:\n",
    "            return self.action_space.sample()\n",
    "        return np.argmax(self.state_action_table[state])\n",
    "\n",
    "\n",
    "class Simulation(object):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Simulates rollouts of an environment, given a policy to follow.\"\"\"\n",
    "        self.env = env\n",
    "\n",
    "    def rollout(self, policy, render=False, explore=True, epsilon=0.1):\n",
    "        \"\"\"Returns experiences for a policy rollout.\"\"\"\n",
    "        experiences = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.get_action(state, explore, epsilon)\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            experiences.append([state, action, reward, next_state])\n",
    "            state = next_state\n",
    "            if render:\n",
    "                time.sleep(0.05)\n",
    "                self.env.render()\n",
    "\n",
    "        return experiences\n",
    "\n",
    "\n",
    "def update_policy(policy, experiences, weight=0.1, discount_factor=0.9):\n",
    "    \"\"\"Updates a given policy with a list of (state, action, reward, state)\n",
    "    experiences.\"\"\"\n",
    "    for state, action, reward, next_state in experiences:\n",
    "        next_max = np.max(policy.state_action_table[next_state])\n",
    "        value = policy.state_action_table[state][action]\n",
    "        new_value = (1 - weight) * value + weight * \\\n",
    "                    (reward + discount_factor * next_max)\n",
    "        policy.state_action_table[state][action] = new_value\n",
    "\n",
    "\n",
    "def train_policy(env, num_episodes=10000, weight=0.1, discount_factor=0.9):\n",
    "    \"\"\"Training a policy by updating it with rollout experiences.\"\"\"\n",
    "    policy = Policy(env)\n",
    "    sim = Simulation(env)\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = sim.rollout(policy)\n",
    "        update_policy(policy, experiences, weight, discount_factor)\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    \"\"\"Evaluate a trained policy through rollouts.\"\"\"\n",
    "    simulation = Simulation(env)\n",
    "    steps = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = simulation.rollout(policy, render=True, explore=False)\n",
    "        steps += len(experiences)\n",
    "\n",
    "    print(f\"{steps / num_episodes} steps on average \"\n",
    "          f\"for a total of {num_episodes} episodes.\")\n",
    "\n",
    "    return steps / num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Tune Flow](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_05/tune_flow.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b77888",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import random\n",
    "search_space = []\n",
    "for i in range(10):\n",
    "    random_choice = {\n",
    "        'weight': random.uniform(0, 1),\n",
    "        'discount_factor': random.uniform(0, 1)\n",
    "    }\n",
    "    search_space.append(random_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53678cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def objective(config):\n",
    "    environment = Environment()\n",
    "    policy = train_policy(\n",
    "        environment,\n",
    "        weight=config[\"weight\"],\n",
    "        discount_factor=config[\"discount_factor\"]\n",
    "    )\n",
    "    score = evaluate_policy(environment, policy)\n",
    "    return [score, config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699de128",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "result_objects = [objective.remote(choice) for choice in search_space]\n",
    "results = ray.get(result_objects)\n",
    "\n",
    "results.sort(key=lambda x: x[0])\n",
    "print(results[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb562ac8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"weight\": tune.uniform(0, 1),\n",
    "    \"discount_factor\": tune.uniform(0, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c076bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_objective(config):\n",
    "    environment = Environment()\n",
    "    policy = train_policy(\n",
    "        environment,\n",
    "        weight=config[\"weight\"],\n",
    "        discount_factor=config[\"discount_factor\"]\n",
    "    )\n",
    "    score = evaluate_policy(environment, policy)\n",
    "\n",
    "    return {\"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b8963",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "analysis = tune.run(tune_objective, config=search_space)\n",
    "print(analysis.get_best_config(metric=\"score\", mode=\"min\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a09df",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "\n",
    "algo = BayesOptSearch(random_search_steps=4)\n",
    "\n",
    "tune.run(\n",
    "    tune_objective,\n",
    "    config=search_space,\n",
    "    metric=\"score\",\n",
    "    mode=\"min\",\n",
    "    search_alg=algo,\n",
    "    stop={\"training_iteration\": 10},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588fccc6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def objective(config):\n",
    "    for step in range(30):\n",
    "        score = config[\"weight\"] * (step ** 0.5) + config[\"bias\"]\n",
    "        tune.report(score=score)\n",
    "\n",
    "\n",
    "search_space = {\"weight\": tune.uniform(0, 1), \"bias\": tune.uniform(0, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de86454",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "\n",
    "\n",
    "scheduler = HyperBandScheduler(metric=\"score\", mode=\"min\")\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    scheduler=scheduler,\n",
    "    num_samples=10,\n",
    ")\n",
    "\n",
    "print(analysis.get_best_config(metric=\"score\", mode=\"min\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddb245",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# NOTE: in the book we have 0.5 GPUs, but set this to 0 here so that it runs on Colab.\n",
    "from ray import tune\n",
    "\n",
    "tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    num_samples=10,\n",
    "    resources_per_trial={\"cpu\": 2, \"gpu\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune import Callback\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "\n",
    "class PrintResultCallback(Callback):\n",
    "    def on_trial_result(self, iteration, trials, trial, result, **info):\n",
    "        print(f\"Trial {trial} in iteration {iteration}, \"\n",
    "              f\"got result: {result['score']}\")\n",
    "\n",
    "\n",
    "def objective(config):\n",
    "    for step in range(30):\n",
    "        score = config[\"weight\"] * (step ** 0.5) + config[\"bias\"]\n",
    "        tune.report(score=score, step=step, more_metrics={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccbcf4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "search_space = {\"weight\": tune.uniform(0, 1), \"bias\": tune.uniform(0, 1)}\n",
    "\n",
    "analysis = tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    mode=\"min\",\n",
    "    metric=\"score\",\n",
    "    callbacks=[PrintResultCallback()])\n",
    "\n",
    "best = analysis.best_trial\n",
    "print(pretty_print(best.last_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefdaab6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# NOTE: this will only run if you insert a correct logdir.\n",
    "analysis = tune.run(\n",
    "    objective,\n",
    "    name=\"<your-logdir>\",\n",
    "    resume=True,\n",
    "    config=search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679008e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    stop={\"training_iteration\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96462e39",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def stopper(trial_id, result):\n",
    "    return result[\"score\"] < 2\n",
    "\n",
    "\n",
    "tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    stop=stopper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91fd6e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import numpy as np\n",
    "\n",
    "search_space = {\n",
    "    \"weight\": tune.sample_from(\n",
    "        lambda context: np.random.uniform(low=0.0, high=1.0)\n",
    "    ),\n",
    "    \"bias\": tune.sample_from(\n",
    "        lambda context: context.config.weight * np.random.normal()\n",
    "    )}\n",
    "\n",
    "tune.run(objective, config=search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2dd055",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# NOTE: this run will take incredibly long on Colab, be warned!\n",
    "from ray import tune\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"DQN\",\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    config={\n",
    "        \"env\": \"CartPole-v1\",\n",
    "        \"lr\": tune.uniform(1e-5, 1e-4),\n",
    "        \"train_batch_size\": tune.choice([10000, 20000, 40000]),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Tune Model Training](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_05/Tune_model_training.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79160b9f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    num_classes = 10\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e71386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from ray.tune.integration.keras import TuneReportCallback\n",
    "\n",
    "\n",
    "def objective(config):\n",
    "    (x_train, y_train), (x_test, y_test) = load_data()\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28)))\n",
    "    model.add(Dense(config[\"hidden\"], activation=config[\"activation\"]))\n",
    "    model.add(Dropout(config[\"rate\"]))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=10,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[TuneReportCallback({\"mean_accuracy\": \"accuracy\"})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1826b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "initial_params = [{\"rate\": 0.2, \"hidden\": 128, \"activation\": \"relu\"}]\n",
    "algo = HyperOptSearch(points_to_evaluate=initial_params)\n",
    "\n",
    "search_space = {\n",
    "    \"rate\": tune.uniform(0.1, 0.5),\n",
    "    \"hidden\": tune.randint(32, 512),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\"])\n",
    "}\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    objective,\n",
    "    name=\"keras_hyperopt_exp\",\n",
    "    search_alg=algo,\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\",\n",
    "    stop={\"mean_accuracy\": 0.99},\n",
    "    num_samples=10,\n",
    "    config=search_space,\n",
    ")\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
