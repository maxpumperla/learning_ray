{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e889b15d",
   "metadata": {},
   "source": [
    "# Chapter 3: Building Your First Distributed Application With Ray Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::discrete_actions[]\n",
    "import random\n",
    "\n",
    "\n",
    "class Discrete:\n",
    "    def __init__(self, num_actions: int):\n",
    "        \"\"\" Discrete action space for num_actions.\n",
    "        Discrete(4) can be used as encoding moving in\n",
    "        one of the cardinal directions.\n",
    "        \"\"\"\n",
    "        self.n = num_actions\n",
    "\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n - 1)  # <1>\n",
    "\n",
    "\n",
    "space = Discrete(4)\n",
    "print(space.sample())  # <2>\n",
    "# end::discrete_actions[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f10893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::init_env[]\n",
    "import os\n",
    "\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    seeker, goal = (0, 0), (4, 4)  # <1>\n",
    "    info = {'seeker': seeker, 'goal': goal}\n",
    "\n",
    "    def __init__(self,  *args, **kwargs):\n",
    "        self.action_space = Discrete(4)  # <2>\n",
    "        self.observation_space = Discrete(5*5)  # <3>\n",
    "# end::init_env[]\n",
    "\n",
    "# tag::env_helpers[]\n",
    "    def reset(self):  # <1>\n",
    "        \"\"\"Reset seeker and goal positions, return observations.\"\"\"\n",
    "        self.seeker = (0, 0)\n",
    "        self.goal = (4, 4)\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"Encode the seeker position as integer\"\"\"\n",
    "        return 5 * self.seeker[0] + self.seeker[1]  # <2>\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Reward finding the goal\"\"\"\n",
    "        return 1 if self.seeker == self.goal else 0  # <3>\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"We're done if we found the goal\"\"\"\n",
    "        return self.seeker == self.goal  # <4>\n",
    "# end::env_helpers[]\n",
    "\n",
    "# tag::env_step[]\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in a direction and return all available information.\"\"\"\n",
    "        if action == 0:  # move down\n",
    "            self.seeker = (min(self.seeker[0] + 1, 4), self.seeker[1])\n",
    "        elif action == 1:  # move left\n",
    "            self.seeker = (self.seeker[0], max(self.seeker[1] - 1, 0))\n",
    "        elif action == 2:  # move up\n",
    "            self.seeker = (max(self.seeker[0] - 1, 0), self.seeker[1])\n",
    "        elif action == 3:  # move right\n",
    "            self.seeker = (self.seeker[0], min(self.seeker[1] + 1, 4))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        obs = self.get_observation()\n",
    "        rew = self.get_reward()\n",
    "        done = self.is_done()\n",
    "        return obs, rew, done, self.info  # <1>\n",
    "# end::env_step[]\n",
    "\n",
    "# tag::env_render[]\n",
    "    def render(self, *args, **kwargs):\n",
    "        \"\"\"Render the environment, e.g. by printing its representation.\"\"\"\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')  # <1>\n",
    "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|S'  # <2>\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))  # <3>\n",
    "# end::env_render[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd536df",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::env_test[]\n",
    "import time\n",
    "\n",
    "environment = Environment()\n",
    "\n",
    "while not environment.is_done():\n",
    "    random_action = environment.action_space.sample()  # <1>\n",
    "    environment.step(random_action)\n",
    "    time.sleep(0.1)\n",
    "    environment.render()  # <2>\n",
    "# end::env_test[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c0304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::policy[]\n",
    "class Policy:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A Policy suggests actions based on the current state.\n",
    "        We do this by tracking the value of each state-action pair.\n",
    "        \"\"\"\n",
    "        self.state_action_table = [\n",
    "            [0 for _ in range(env.action_space.n)]\n",
    "            for _ in range(env.observation_space.n)  # <1>\n",
    "        ]\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def get_action(self, state, explore=True, epsilon=0.1):  # <2>\n",
    "        \"\"\"Explore randomly or exploit the best value currently available.\"\"\"\n",
    "        if explore and random.uniform(0, 1) < epsilon:  # <3>\n",
    "            return self.action_space.sample()\n",
    "        return np.argmax(self.state_action_table[state])  # <4>\n",
    "# end::policy[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af9de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::simulation[]\n",
    "class Simulation(object):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Simulates rollouts of an environment, given a policy to follow.\"\"\"\n",
    "        self.env = env\n",
    "\n",
    "    def rollout(self, policy, render=False, explore=True, epsilon=0.1):  # <1>\n",
    "        \"\"\"Returns experiences for a policy rollout.\"\"\"\n",
    "        experiences = []\n",
    "        state = self.env.reset()  # <2>\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.get_action(state, explore, epsilon)  # <3>\n",
    "            next_state, reward, done, info = self.env.step(action)  # <4>\n",
    "            experiences.append([state, action, reward, next_state])  # <5>\n",
    "            state = next_state\n",
    "            if render:  # <6>\n",
    "                time.sleep(0.05)\n",
    "                self.env.render()\n",
    "\n",
    "        return experiences\n",
    "# end::simulation[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::naive_rollout[]\n",
    "untrained_policy = Policy(environment)\n",
    "sim = Simulation(environment)\n",
    "\n",
    "exp = sim.rollout(untrained_policy, render=True, epsilon=1.0)  # <1>\n",
    "for row in untrained_policy.state_action_table:\n",
    "    print(row)  # <2>\n",
    "# end::naive_rollout[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df50bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::update_policy[]\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def update_policy(policy, experiences, weight=0.1, discount_factor=0.9):\n",
    "    \"\"\"Updates a given policy with a list of (state, action, reward, state)\n",
    "    experiences.\"\"\"\n",
    "    for state, action, reward, next_state in experiences:  # <1>\n",
    "        next_max = np.max(policy.state_action_table[next_state])  # <2>\n",
    "        value = policy.state_action_table[state][action]  # <3>\n",
    "        new_value = (1 - weight) * value + weight * \\\n",
    "                    (reward + discount_factor * next_max)  # <4>\n",
    "        policy.state_action_table[state][action] = new_value  # <5>\n",
    "# end::update_policy[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a94869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::train_policy[]\n",
    "def train_policy(env, num_episodes=10000, weight=0.1, discount_factor=0.9):\n",
    "    \"\"\"Training a policy by updating it with rollout experiences.\"\"\"\n",
    "    policy = Policy(env)\n",
    "    sim = Simulation(env)\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = sim.rollout(policy)  # <1>\n",
    "        update_policy(policy, experiences, weight, discount_factor)  # <2>\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "trained_policy = train_policy(environment)  # <3>\n",
    "# end::train_policy[]\n",
    "\n",
    "# Printing gray-scale heatmaps of the state-action value function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PLOT = False\n",
    "if PLOT:\n",
    "    table = trained_policy.state_action_table\n",
    "    for i, direction in enumerate([\"down\", \"left\", \"up\", \"right\"]):\n",
    "        direction_matrix = np.array([table[j][i] for j in range(25)]).reshape(5, 5)\n",
    "        print(direction_matrix)\n",
    "        plt.imshow(direction_matrix, cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::evaluate_policy[]\n",
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    \"\"\"Evaluate a trained policy through rollouts.\"\"\"\n",
    "    simulation = Simulation(env)\n",
    "    steps = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = simulation.rollout(policy, render=True, explore=False)  # <1>\n",
    "        steps += len(experiences)  # <2>\n",
    "\n",
    "    print(f\"{steps / num_episodes} steps on average \"\n",
    "          f\"for a total of {num_episodes} episodes.\")\n",
    "\n",
    "    return steps / num_episodes\n",
    "\n",
    "\n",
    "evaluate_policy(environment, trained_policy)\n",
    "# end::evaluate_policy[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_policy_simulation[]\n",
    "import ray\n",
    "\n",
    "ray.init()\n",
    "environment = Environment()\n",
    "env_ref = ray.put(environment)  # <1>\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def create_policy():\n",
    "    env = ray.get(env_ref)\n",
    "    return Policy(env)  # <2>\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class SimulationActor(Simulation):  # <3>\n",
    "    \"\"\"Ray actor for a Simulation.\"\"\"\n",
    "    def __init__(self):\n",
    "        env = ray.get(env_ref)\n",
    "        super().__init__(env)\n",
    "# end::ray_policy_simulation[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af61e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_training[]\n",
    "@ray.remote\n",
    "def update_policy_task(policy_ref, experiences_list):\n",
    "    \"\"\"Remote Ray task for updating a policy with experiences in parallel.\"\"\"\n",
    "    [update_policy(policy_ref, ray.get(xp)) for xp in experiences_list]  # <1>\n",
    "    return policy_ref\n",
    "\n",
    "\n",
    "def train_policy_parallel(num_episodes=1000, num_simulations=4):\n",
    "    \"\"\"Parallel policy training function.\"\"\"\n",
    "    policy = create_policy.remote()  # <2>\n",
    "    simulations = [SimulationActor.remote() for _ in range(num_simulations)]  # <3>\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = [sim.rollout.remote(policy) for sim in simulations]  # <4>\n",
    "        policy = update_policy_task.remote(policy, experiences)  # <5>\n",
    "\n",
    "    return ray.get(policy)  # <6>\n",
    "# end::ray_training[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09688af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_evaluation[]\n",
    "parallel_policy = train_policy_parallel()\n",
    "evaluate_policy(environment, parallel_policy)\n",
    "# end::ray_evaluation[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cc4742",
   "metadata": {},
   "source": [
    "# ![Task dependency](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_03/train_policy.png)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
