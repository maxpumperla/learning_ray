{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89466d94",
   "metadata": {},
   "source": [
    "# Distributed Training with Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23b3af",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_07_train.ipynb).\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_07_train.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e05d4f",
   "metadata": {},
   "source": [
    "For this chapter you will need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray[data,train]==2.2.0\" \"dask==2022.2.0\" \"torch==1.12.1\"\n",
    "! pip install \"xgboost==1.6.2\" \"xgboost-ray>=0.1.10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef63a5",
   "metadata": {},
   "source": [
    "\n",
    "To import utility files for this chapter, on Colab you will also have to clone\n",
    "the repo and copy the code files to the base path of the runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a179146",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/maxpumperla/learning_ray\n",
    "%cp -r learning_ray/notebooks/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Data Model Parallel](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_07/data_model_parallel.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Torch Trainer](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_07/torch_trainer.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Train Architecture](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_07/train_architecture.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Train Overview](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_07/train_overview.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Train Tune Execution](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_07/train_tune_execution.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409cb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "LABEL_COLUMN = \"is_big_tip\"\n",
    "FEATURE_COLUMNS = [\"passenger_count\", \"trip_distance\", \"fare_amount\",\n",
    "                   \"trip_duration\", \"hour\", \"day_of_week\"]\n",
    "\n",
    "enable_dask_on_ray()\n",
    "\n",
    "\n",
    "def load_dataset(path: str, *, include_label=True):\n",
    "    columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\",\n",
    "               \"passenger_count\", \"trip_distance\", \"fare_amount\"]\n",
    "    df = dd.read_parquet(path, columns=columns)\n",
    "\n",
    "    df = df.dropna()\n",
    "    df = df[(df[\"passenger_count\"] <= 4) &\n",
    "            (df[\"trip_distance\"] < 100) &\n",
    "            (df[\"fare_amount\"] < 1000)]\n",
    "\n",
    "    df[\"tpep_pickup_datetime\"] = dd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "    df[\"tpep_dropoff_datetime\"] = dd.to_datetime(df[\"tpep_dropoff_datetime\"])\n",
    "\n",
    "    df[\"trip_duration\"] = (df[\"tpep_dropoff_datetime\"] -\n",
    "                           df[\"tpep_pickup_datetime\"]).dt.seconds\n",
    "    df = df[df[\"trip_duration\"] < 4 * 60 * 60] # 4 hours.\n",
    "    df[\"hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"tpep_pickup_datetime\"].dt.weekday\n",
    "\n",
    "    if include_label:\n",
    "        df[LABEL_COLUMN] = df[\"tip_amount\"] > 0.2 * df[\"fare_amount\"]\n",
    "\n",
    "    df = df.drop(\n",
    "        columns=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\"]\n",
    "    )\n",
    "\n",
    "    return ray.data.from_dask(df).repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FarePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(6, 256)\n",
    "        self.fc2 = nn.Linear(256, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd15858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air import session\n",
    "from ray.air.config import ScalingConfig\n",
    "import ray.train as train\n",
    "from ray.train.torch import TorchCheckpoint, TorchTrainer\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config: dict):\n",
    "    batch_size = config.get(\"batch_size\", 32)\n",
    "    lr = config.get(\"lr\", 1e-2)\n",
    "    num_epochs = config.get(\"num_epochs\", 3)\n",
    "\n",
    "    dataset_shard = session.get_dataset_shard(\"train\")\n",
    "\n",
    "    model = FarePredictor()\n",
    "    dist_model = train.torch.prepare_model(model)\n",
    "\n",
    "    loss_function = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(dist_model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in dataset_shard.iter_torch_batches(\n",
    "                batch_size=batch_size, dtypes=torch.float\n",
    "        ):\n",
    "            labels = torch.unsqueeze(batch[LABEL_COLUMN], dim=1)\n",
    "            inputs = torch.cat(\n",
    "                [torch.unsqueeze(batch[f], dim=1) for f in FEATURE_COLUMNS], dim=1\n",
    "            )\n",
    "            output = dist_model(inputs)\n",
    "            batch_loss = loss_function(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_batches += 1\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "        session.report(\n",
    "            {\"epoch\": epoch, \"loss\": loss},\n",
    "            checkpoint=TorchCheckpoint.from_model(dist_model)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209607a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# NOTE: In the book we use num_workers=2, but reduce this here, so that it runs on Colab.\n",
    "# In any case, this training loop will take considerable time to run.\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\n",
    "        \"lr\": 1e-2, \"num_epochs\": 3, \"batch_size\": 64\n",
    "    },\n",
    "    scaling_config=ScalingConfig(num_workers=1, resources_per_worker={\"CPU\": 1, \"GPU\": 0}),\n",
    "    datasets={\n",
    "        \"train\": load_dataset(\"nyc_tlc_data/yellow_tripdata_2020-01.parquet\")\n",
    "    },\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "trained_model = result.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88deaa0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "\n",
    "batch_predictor = BatchPredictor(trained_model, TorchPredictor)\n",
    "ds = load_dataset(\n",
    "    \"nyc_tlc_data/yellow_tripdata_2021-01.parquet\", include_label=False)\n",
    "\n",
    "batch_predictor.predict_pipelined(ds, blocks_per_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ray.data import from_torch\n",
    "\n",
    "num_samples = 20\n",
    "input_size = 10\n",
    "layer_size = 15\n",
    "output_size = 5\n",
    "num_epochs = 3\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_data():\n",
    "    return torch.randn(num_samples, input_size)\n",
    "\n",
    "\n",
    "input_data = train_data()\n",
    "label_data = torch.randn(num_samples, output_size)\n",
    "train_dataset = from_torch(input_data)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loss_fn, optimizer):\n",
    "    output = model(input_data)\n",
    "    loss = loss_fn(output, label_data)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def training_loop():\n",
    "    model = NeuralNetwork()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf11c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import prepare_model\n",
    "\n",
    "\n",
    "def distributed_training_loop():\n",
    "    model = NeuralNetwork()\n",
    "    model = prepare_model(model)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc5a35",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=distributed_training_loop,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=2,\n",
    "        use_gpu=False\n",
    "    ),\n",
    "    datasets={\"train\": train_dataset}\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c049af7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray import tune\n",
    "from ray.data.preprocessors import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "dataset = ray.data.from_items(\n",
    "    [{\"X\": x, \"Y\": 1} for x in range(0, 100)] +\n",
    "    [{\"X\": x, \"Y\": 0} for x in range(100, 200)]\n",
    ")\n",
    "prep_v1 = StandardScaler(columns=[\"X\"])\n",
    "prep_v2 = MinMaxScaler(columns=[\"X\"])\n",
    "\n",
    "param_space = {\n",
    "    \"scaling_config\": ScalingConfig(\n",
    "        num_workers=tune.grid_search([2, 4]),\n",
    "        resources_per_worker={\n",
    "            \"CPU\": 2,\n",
    "            \"GPU\": 0,\n",
    "        },\n",
    "    ),\n",
    "    \"preprocessor\": tune.grid_search([prep_v1, prep_v2]),\n",
    "    \"params\": {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "        \"eta\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"subsample\": tune.uniform(0.5, 1.0),\n",
    "        \"max_depth\": tune.randint(1, 9),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e305d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.air.config import RunConfig\n",
    "from ray.tune import Tuner\n",
    "\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    params={},\n",
    "    run_config=RunConfig(verbose=2),\n",
    "    preprocessor=None,\n",
    "    scaling_config=None,\n",
    "    label_column=\"Y\",\n",
    "    datasets={\"train\": dataset}\n",
    ")\n",
    "\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space=param_space,\n",
    ")\n",
    "\n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
